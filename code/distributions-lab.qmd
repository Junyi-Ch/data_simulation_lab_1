---
title: "Simulations & Tidyverse lab"
subtitle: "Psych 201a"
author: "Bria Long"
date: "`r format(Sys.Date())`"
format:
  html:
    toc: true
    toc-depth: 2
    theme: cosmo
  pdf: default
execute:
  echo: true
  warning: false
  message: false
editor: visual
---

> **Goals:** (1) Understand how normal, binomial, and lognormal distributions describe behavioral data (accuracy, RTs, individual variability). (2) Practice generating and visualizing synthetic datasets using tidyverse functions. (3) Learn how to write reproducible lab reports with inline statistics in Quarto.

> **Workflow:** Copy the worked example, then complete tasks using tidyverse verbs. Keep your write-up reproducible using **inline R**.

## Setup

```{r}
#| label: setup
library(tidyverse)
set.seed(2025)

```

## A. Normal distribution (worked example)

Here, let's try to simulate some values from a normal distribution with a mean of 0 and a SD of 1.

The `rnorm` function is used below here.

```{r}

# you can play around with these
n <- 300 # number of datapoints
mu <- 0 # mean of the distribution
sd_norm <- 1 # sd of the distribution

# rnorm is your simulation function here
# take a look at how it's used by typing ``? rnorm``
## note that this function is taking the inputs you specified above -- it doesn't matter what these are called
norm_df <- tibble(sim_values = rnorm(n, mu, sd_norm))

# so you know, this code would do the same thing
# norm_df <- tibble(sim_values = rnorm(300, 0, 1))

# okay, check out the top of this dataframe
head(norm_df)
```

Calculate some basic summaries of this dataframe and print them out

```{r}
# TODO: Calculate mean, sd, and n for norm_df
# Hint: Use summarise() with mean(), sd(), and n()

# What happens if you increase the "n"? How do these summaries change?
```

I'm going to now plot a histogram of these values. We're going to spend a lot of time on figuring out how to plot things later -- but here you can start learning the basics.

The first argument to the ggplot function is your dataframe -- here, `norm_df`. The `aes` is where you specify what is on the x-axis, y-axis, colors, etc. These values come from the columns in `norm_df`.

`Alpha` is transparency. `binwidth` is how big the histogram bins are.

```{r}
ggplot(norm_df, aes(x=sim_values)) +
  geom_histogram(binwidth = 0.25, alpha = 0.85) +
  labs(title = "Simulated values", x = "Value", y = "Count")
```

**Exercise:** change `mu` and `sd_norm` to two other combos (e.g., 10 & 2; 10 & 5).

Write one sentence with an inline mean (e.g., `r round(mean(norm_df$sim_values), 2)`).

Bonus: Compute another kind of summary statistic on the dataset Bonus: Simulate another distribution and also plot it on the same histogram (you can make it a different color)

## B. Binomial data (accuracy)

Okay, normally distributed data is great, but often not what we're working with in practice. Often, we have accuracies or reaction times.

First, we'll think about accuracy. To do so, we'll practice `rbinom()` and how to summarize it using some tidyverse functions. You should be able to adapt the code we just worked through but with a new function --

### B1 --- Bernoulli accuracy

Tasks: - Simulate `n = 200` 0/1 outcomes with `p = 0.92` into `acc_df`. - Compute **proportion correct** and a **count table** with `count()`. - Plot overall proportion with `stat_summary()`.

```{r}
#| label: binom-bernoulli
# TODO: create a simulated dataframe using rbinom (remember, you can look at the help for the function)
# Hint: Use tibble() and rbinom(n=200, size=1, prob=0.92)

# TODO: summarise and count
# Hint: Use summarise() to calculate proportion correct and n
```

```{r}
## TO DO: Make a histogram of these values
# Hint: Use ggplot() with geom_histogram()
```

```{r}
# TODO: plot overall proportion 
# Hint:  use `stat_summary(fun = mean)` for the mean

# TODO: Plot some measure of variability (SD or 95% CI)
# Hint: stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.1) +
# You have to give stat_summary an x-value, but since there isn't really one here you can just give it a label (e.g., "overall")
```

### B2 --- Two conditions + blocks (tidy)

Tasks: - Build a tibble with `condition` in {easy, hard}, 100 trials each. - Use different probabilities (e.g., 0.97 vs 0.87). - Summarise `prop_correct` by `condition × block` and `pivot_wider()`. - Bonus: Add a `block` or `participant` factor

```{r}
#| label: binom-conditions-blocks
# TODO: simulate
# Hint: Create a tibble with condition, block, and acc columns
# Use rep() to repeat conditions and blocks appropriately
# Use rbinom() with different probabilities for easy vs hard conditions

# TODO: Check per-block counts
# Hint: Use add_count() and distinct()

# TODO: Summaries by condition × block
# Hint: Use group_by() and summarise()

# TODO: Pivot wider
# Hint: Use pivot_wider() to make conditions into separate columns
```

Inline prompt: "Overall accuracy = `r NA`. Easy = `r NA`, Hard = `r NA`." Replace NA with inline code.

## C. Reaction times (RT): shifted lognormal

RTs are famously right-skewed (ie., sometimes participants take a very long time to respond). This looks reallly different than a normal distribution. We'll use `t0 + LogNormal(μ, σ)` to approximately simulate their distribution `t0` can be thought of a "baseline" reaction time that could vary by participant.

### Simulating reaction times

Tasks: - Simulate **n = 500** RTs with `t0 = 300`, `meanlog = log(250)`, `sdlog = 0.35` into `rt_df`. - Plot a histogram and compute **mean**, **median**, **max**. - Create `log_rt = log(rt)` and compare distributions

Here's a helper function to create lognormal RTs with a baseline shift. Note on these parameters: In a lognormal distribution, the parameters `meanlog` and `sdlog` describe the underlying normal distribution on the log scale, NOT the mean and standard deviation of the observed reaction times. If we want RTs centered around 250 ms, we set `meanlog` = log(250) because the median of a lognormal is `exp(meanlog)`---so this choice makes the median RT about 250 ms. The `sdlog` parameter controls the amount of skew on the log scale: typical behavioral data are well-modeled by `sdlog` values between about 0.3 and 0.5. Using `sdlog` = log(20) would be incorrect, because \`sdlog already represents variability on the log scale; taking the log of a raw-scale SD would produce unrealistically long-tailed RTs spanning thousands of milliseconds. You don't need to understand all of these details, but I realize it could be confusing and so wanted to include this explanation.

```{r}
# Helper: shifted lognormal RTs (t0 + LogNormal(meanlog, sdlog))
r_shifted_lognorm <- function(n, t0, meanlog, sdlog) {
  t0 + rlnorm(n, meanlog = meanlog, sdlog = sdlog)
}
```

```{r}
#| label: lognormal-single
# TODO: Simulate RTs and their logged RTs
# Hint: Use the r_shifted_lognorm() function with the specified parameters
# Create a tibble with rt and log_rt columns
```

Now look at their relationship -- hint is to use qplot, or you can use ggplot. You'll need the `$` operator (e.g., `rt_df$rt`) to grab a particular column from a dataframe in base R.

```{r}
# TODO: Plot rt vs log_rt
# Hint: Use qplot() or ggplot()
```

OK, now let's make some summaries. Use the summarize function to calculate the mean, median, max, and sd of the rt distribution. Note that you don't have to make a new dataframe for this.

```{r}
# TODO: summaries
# Hint: Use summarise() to calculate mean_rt, median_rt, max_rt, sd_rt
```

## D. Two conditions: valid vs invalid cue

Backstory. In a spatial attention task (think Posner cueing), each trial begins with a cue that is either valid (points to the upcoming target location) or invalid (points away). Valid cues help orient attention and typically speed responses; invalid cues slow responses. Reaction times (RTs) are right-skewed, as in the prior example. Let's try to simulate valid and invalid trials.

Add a **condition** factor. Suppose invalid cues slow responses by \~**40 ms** on average.

### Try it

```{r}
# TODO: Simulate RTs for valid and invalid conditions
# Hint: Use r_shifted_lognorm() with different parameters for each condition
# Invalid cues should be slower (higher meanlog)
```

Use `pivot_longer` to make this a long dataframe, where `rt` is just one column, and you have `condition` in another column. Use the `?` to look at the arguments for the function.

```{r}
# TODO: Use pivot_longer to reshape the data
# Hint: Convert rt_valid and rt_invalid columns into rt and condition columns
```

Now, use `group_by(condition)` before commuting your summaries in tidyverse -- you should see outputs grouped by condition.

```{r}

# TODO: group_by() condition and summarise mean_rt, sd_rt
# Hint: Use group_by(condition) then summarise()
```

Now, make some sort of visual of the two distributions. Give the raw dataframe to ggplot, and use `geom_density` to plot the density (you can toggle `alpha` here to change the transparency). Use `fill=condition` within `aes` to specify that the two conditions should have different colors.

```{R}
## TODO: Use ggplot to make density plot of simulated data
# Hint: Use geom_density() with fill=condition in aes()
```

BONUS. Okay, so far you've been simulating "raw" data -- i.e., 500 trials of valid, and 500 trials of invalid cueing. Try simulating 10 different participants, where each participant completes 200 trials.

```{r}
# TODO: Simulate multiple participants
# Hint: Use a for loop to iterate over participants
# Each participant should have different baseline shifts
# Use bind_rows() to combine all participant data
```

```{r}
# TODO: Summarize by participant and condition
# Hint: Use group_by(participant, condition) and summarise()
```

BONUS: Plot average RTs and 95 CIs over individual subjects. You can reuse `stat_summary` arguments for the mean and CIs from above. Plot raw data points from each participant using `geom_point`.

```{r}
# TODO: Plot average RTs and 95 CIs over individual subjects
# Hint: Use stat_summary() for means and CIs, geom_point() for individual data
```